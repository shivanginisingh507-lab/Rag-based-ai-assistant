![RAG Demo](./demo.gif)

# ğŸš€ RAG-Based AI Teaching Assistant  
### Video â†’ Transcription â†’ Embeddings â†’ Intelligent Q&A

A full-stack **Retrieval-Augmented Generation (RAG)** system that allows users to upload videos and ask intelligent questions based on their content.

Built with:

- âš¡ FastAPI
- ğŸŸ¢ Next.js
- ğŸ³ Docker
- ğŸ§  Ollama (bge-m3 embeddings)
- ğŸ™ Whisper (Speech-to-Text)
- ğŸ¤– Gemini API
- â˜ï¸ AWS EC2 (Elastic IP Deployment) 

---

# ğŸ— Architecture Overview

## ğŸ”¹ High-Level System Architecture

```mermaid
flowchart TD
    Browser["ğŸ‘©â€ğŸ’» User Browser"]
    EC2["â˜ï¸ AWS EC2 Instance (Elastic IP)"]

    subgraph Docker Network
        Frontend["ğŸŸ¢ rag_frontend\nNext.js :3000"]
        Backend["ğŸ”µ rag_backend\nFastAPI :8000"]
        Ollama["ğŸŸ£ rag_ollama\nOllama :11434 (Internal)"]
    end

    Browser -->|HTTP :3000| Frontend
    Frontend -->|API Call :8000| Backend
    Backend -->|Internal Docker Network| Ollama
```

---


## ğŸŒ Network Flow (Public vs Internal)

```mermaid
flowchart LR
    Browser -->|"ELASTIC IP:3000"| Frontend
    Frontend -->|"ELASTIC IP:8000"| Backend
    Backend -->|"http://ollama:11434"| Ollama
```

---

# ğŸ§  RAG Processing Pipeline

```mermaid
flowchart LR

    Video["ğŸ“¹ Uploaded Video"]
    Audio["ğŸµ Extract Audio (FFmpeg)"]
    Whisper["ğŸ“ Whisper Transcription"]
    JSON["ğŸ“„ Chunked JSON"]
    Embed["ğŸ”¢ Generate Embeddings (bge-m3)"]
    Vector["ğŸ’¾ embeddings.joblib"]
    Query["â“ User Question"]
    Retrieve["ğŸ” Similarity Search"]
    LLM["ğŸ¤– Gemini Response"]

    Video --> Audio
    Audio --> Whisper
    Whisper --> JSON
    JSON --> Embed
    Embed --> Vector
    Query --> Retrieve
    Vector --> Retrieve
    Retrieve --> LLM
```

---

# âœ¨ Features

- ğŸ¥ Upload your videos
- ğŸ™ Automatic audio extraction using FFmpeg
- ğŸ“ Speech-to-text transcription using Whisper
- ğŸ§© Intelligent text chunking
- ğŸ”¢ Embedding generation using bge-m3
- ğŸ’¾ Persistent vector storage
- â“ Ask contextual questions
- ğŸ¤– Gemini-powered intelligent responses
- ğŸ³ Fully containerized architecture
- â˜ï¸ Deployable on AWS EC2

---

# ğŸš€ Deployment (AWS EC2 with Elastic IP)

## 1ï¸âƒ£ Launch EC2

- Instance: t3.large (8GB RAM minimum)
- OS: Ubuntu 24.04 LTS
- Open Ports:
  - 22 (SSH)
  - 3000 (Frontend)
  - 8000 (Backend)

---

## 2ï¸âƒ£ Install Docker

```bash
sudo apt update && sudo apt upgrade -y
sudo apt install -y docker.io docker-compose-plugin
sudo systemctl enable docker
sudo systemctl start docker
sudo usermod -aG docker ubuntu
newgrp docker
```

---

## 3ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/your-username/Rag-based-ai-assistant.git
cd Rag-based-ai-assistant
```

---

## 4ï¸âƒ£ Configure Environment Variables

### Backend (`backend/.env`)

```
GEMINI_API_KEY=your_key
```

### Frontend (`frontend/.env.production`)

```
NEXT_PUBLIC_API_URL=http://<YOUR_ELASTIC_IP>:8000
```

### Root (`.env`)

```
NEXT_PUBLIC_API_URL=http://<YOUR_ELASTIC_IP>:8000
```

---

## 5ï¸âƒ£ Start Application

```bash
docker compose up -d --build
```

Verify:

```bash
docker ps
```

---

## 6ï¸âƒ£ Access Application

Frontend:

```
http://<YOUR_ELASTIC_IP>:3000
```

Backend Docs:

```
http://<YOUR_ELASTIC_IP>:8000/docs
```

---

# ğŸ“‚ Project Structure

```
Rag-based-ai-assistant/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Dockerfile
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ Dockerfile
â”‚
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---
# ğŸŒ Live Deployment Links

My public links:

### ğŸ”¹ Backend (Swagger Docs)
```
http://54.79.153.24:8000/docs
```

### ğŸ”¹ Frontend Application
```
http://54.79.153.24:3000/
```

---

# ğŸ‘©â€ğŸ’» Author

Shivangini Singh  
Full Stack Developer | AI Systems Builder

---

# â­ If You Like This Project

Give it a â­ on GitHub!
